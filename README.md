# MVP: RAG-агент для 3-й линии поддержки

Локальный RAG-помощник для быстрого поиска похожих инцидентов Naumen на основе тикетов из CSV/XLSX. Все компоненты работают оффлайн на CPU и используют open-source стек.

## Возможности
- Загрузка тикетов из CSV/XLSX (обязательно поля `ID`, `Описание`, `Решение`; опционально `Дата`, `Статус`, `Тип`).
- Очистка HTML/технического шума и склейка «Описание + Решение».
- Разбиение на чанки по 500 символов с перекрытием.
- Построение эмбеддингов с помощью `intfloat/multilingual-e5-small` (можно заменить).
- Хранение векторного индекса FAISS + payload с идентификаторами тикетов.
- CLI для индексации, поиска похожих инцидентов и дозаписи новых тикетов в существующий индекс.
- Streamlit-UI для загрузки файлов, обновления индекса и просмотра текущего состояния.

## Быстрый старт
### 1) Установка окружения
```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### 2) Индексация тикетов
На вход подаётся CSV/XLSX c колонками:
- `ID` — идентификатор тикета (обязательно)
- `Описание` — текст обращения
- `Решение` — текст решения
- `Дата`, `Статус`, `Тип` — необязательно

Если экспорт из Naumen содержит другие заголовки, они автоматически сопоставляются:
- `Номер запроса` → `ID`
- `Описание` → `Описание`
- `Описание решения` → `Решение`
- `Дата/время регистрации` → `Дата`
- `Текущий статус` → `Статус`
- `Вид запроса` → `Тип`

Пример индексирования демо-файла (по умолчанию инференс только на CPU):
```bash
python -m rag_agent.ingest \
  --input data/sample_tickets.csv \
  --output data/index \
  --model intfloat/multilingual-e5-small \
  --chunk-size 500 \
  --chunk-overlap 50 \
  --device cpu
```
В результате появятся файлы `index.faiss`, `metadata.jsonl` и `config.json` в папке `data/index`.

### 3) Поиск похожих тикетов
```bash
python -m rag_agent.query \
  --index data/index \
  --query "Ошибка авторизации в личном кабинете" \
  --top-k 5 \
  --device cpu
```
Выводит топ-N релевантных чанков с привязкой к `ticket_id` и `chunk_id`, готовых для подстановки в RAG-промпт.

### 4) Дозапись новых тикетов в существующий индекс
```bash
python -m rag_agent.update_index \
  --input data/new_batch.csv \
  --index data/index \
  --device cpu
```
Скрипт добавит только новые `ID`; существующие будут пропущены и показаны в отчёте.

### 5) Streamlit-интерфейс
```bash
streamlit run streamlit_app.py
```
UI:
- проверяет файл CSV/XLSX и показывает присутствующие/отсутствующие колонки;
- сравнивает ID тикетов в файле с уже проиндексированными и добавляет только новые (upsert);
- архивирует загруженный файл в `data/uploaded/` и пишет отчёты/ошибки в `data/logs/updates.log`;
- умеет создать индекс в `data/index/`, если его ещё нет (требуется указать модель в UI).

## Архитектура
- **Эмбеддинги**: `sentence-transformers` (по умолчанию `intfloat/multilingual-e5-small`).
- **Векторное хранилище**: локальный FAISS IndexFlatIP с нормализацией (косинусное сходство).
- **Метаданные**: `metadata.jsonl` с полями `ticket_id`, `chunk_id`, `text`, `source_fields`.
- **Конфигурация**: `config.json` (модель, размерность, параметры чанков, устройство инференса).

## Формат payload для RAG
Каждый найденный чанк содержит:
```json
{
  "ticket_id": "12345",
  "chunk_id": 0,
  "text": "...",
  "source_fields": {
    "date": "2024-01-10",
    "status": "Закрыт",
    "type": "Инцидент"
  }
}
```

## Пример интеграции в промпт
```text
Вы оператор 3-й линии. Используйте контекст ниже, чтобы ответить на вопрос. Указывайте ID тикетов.

Вопрос: <вопрос оператора>

Контекст:
- [ID=<ticket_id> #<chunk_id>] <текст чанка>
...
```

## Локальная работа и замены
- Для полностью оффлайн-окружения скачайте модель заранее: `python -m sentence_transformers.download intfloat/multilingual-e5-small`.
- Модель можно заменить на любую совместимую из `sentence-transformers` — укажите её через флаг `--model` при индексации и поиске.
- Вместо FAISS можно подключить Qdrant: реализуйте адаптер в `rag_agent/pipeline.py` по аналогии с FAISS (класс `VectorStore`).

## Структура репозитория
```
rag_agent/
  pipeline.py      # функции очистки, чанкинга, эмбеддинга, работы с FAISS
  ingest.py        # CLI для индексации
  query.py         # CLI для поиска
  cli.py           # объединённый интерфейс (опционально)
```

## Ограничения и будущая работа
- Ответы сейчас без генерации LLM; интеграция любой локальной модели возможна после retrieval.
```
